// Copyright (C) 2024 Intel Corporation
// SPDX-License-Identifier: Apache-2.0

namespace MathSolver.Models{
    #region Using
    using System.Runtime.Serialization;
    using System.Text.Json.Serialization;
    #endregion

    [DataContract]
    public class ChatCompletionRequest
    {
        // Ordered by official OpenAI API documentation
        // https://platform.openai.com/docs/api-reference/chat/create

        /// <summary>
        ///  A list of messages comprising the conversation so far. Depending on the model you use, different message
        /// types (modalities) are supported
        /// </summary>
        [JsonPropertyName("messages")]
        public List<Dictionary<string, string>>? Messages { get; set; } // Can be string or List<Dictionary<string, object>>

        /// <summary>
        /// ID of the model to use.
        /// </summary>
        [JsonPropertyName("model")]
        public string Model { get; set; } = string.Empty;

        /// <summary>
        /// Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the
        /// text so far, decreasing the model's likelihood to repeat the same line verbatim.
        /// </summary>
        [JsonPropertyName("frequency_penalty")]
        public float? FrequencyPenalty { get; set; } = (float)0.0;

        /// <summary>
        /// Modify the likelihood of specified tokens appearing in the completion.
        /// Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias
        /// value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to
        /// sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase
        /// likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the
        /// relevant token.
        /// </summary>
        [JsonPropertyName("logit_bias")]
        public Dictionary<string, float>? LogitBias { get; set; }

        /// <summary>
        /// Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities
        // of each output token returned in the content of message.
        /// </summary>
        [JsonPropertyName("logprobs")]
        public bool? Logprobs { get; set; } = false;

        /// <summary>
        /// An integer between 0 and 20 specifying the number of most likely tokens to return at each token position,
        /// each with an associated log probability. logprobs must be set to true if this parameter is used.
        /// </summary>
        [JsonPropertyName("top_logprobs")]
        public int? TopLogprobs { get; set; } = 0;

        /// <summary>
        /// The maximum number of tokens that can be generated in the chat completion. This value can be used to
        /// control costs for text generated via API.
        /// </summary>
        [JsonPropertyName("max_tokens")]
        public int? MaxTokens { get; set; } = 1024;

        /// <summary>
        /// How many chat completion choices to generate for each input message. Note that you will be charged based on
        /// the number of generated tokens across all of the choices. Keep n as 1 to minimize costs.
        /// </summary>
        [JsonPropertyName("n")]
        public int N { get; set; } = 1;

        /// <summary>
        /// Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text
        /// so far, increasing the model's likelihood to talk about new topics.
        /// </summary>
        [JsonPropertyName("presence_penalty")]
        public double? PresencePenalty { get; set; } = 0.0;

        /// <summary>
        /// An object specifying the format that the model must output.
        /// Setting to { "type": "json_schema", "json_schema": {...} } enables Structured Outputs which ensures the
        /// model will match your supplied JSON schema. Learn more in the Structured Outputs guide.
        /// Setting to { "type": "json_object" } enables JSON mode, which ensures the message the model generates is
        /// valid JSON.
        /// Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system
        /// or user message. Without this, the model may generate an unending stream of whitespace until the generation
        /// reaches the token limit, resulting in a long-running and seemingly "stuck" request.
        /// Also note that the message content may be partially cut off if finish_reason="length", which indicates the
        /// generation exceeded max_tokens or the conversation exceeded the max context length.
        /// </summary>
        [JsonPropertyName("response_format")]
        public string? ResponseFormat { get; set; }

        /// <summary>
        /// This feature is in Beta. If specified, our system will make a best effort to sample deterministically,
        /// such that repeated requests with the same seed and parameters should return the same result. Determinism
        /// is not guaranteed, and you should refer to the system_fingerprint response parameter to monitor changes in
        /// the backend.
        /// </summary>
        [JsonPropertyName("seed")]
        public long Seed { get; set; }

        /// <summary>
        /// Specifies the latency tier to use for processing the request. This parameter is relevant for customers
        /// subscribed to the scale tier service:
        /// ->  If set to 'auto', and the Project is Scale tier enabled, the system will utilize scale tier credits
        ///     until they are exhausted.
        /// ->  If set to 'auto', and the Project is not Scale tier enabled, the request will be processed using the
        ///     default service tier with a lower uptime SLA and no latency guarantee.
        /// ->  If set to 'default', the request will be processed using the default service tier with a lower uptime
        ///     SLA and no latency guarantee.
        /// ->  When not set, the default behavior is 'auto'.
        /// </summary>
        [JsonPropertyName("service_tier")]
        public string? ServiceTier { get; set; }

        /// <summary>
        /// Up to 4 sequences where the API will stop generating further tokens.
        /// </summary>
        [JsonPropertyName("stop")]
        public List<string> Stop { get; set; } = new List<string>();

        /// <summary>
        /// If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only server-sent
        /// events as they become available, with the stream terminated by a data: [DONE] message.
        /// </summary>
        [JsonPropertyName("stream")]
        public bool? Stream { get; set; } = false;

        /// <summary>
        /// Options for streaming response. Only set this when you set stream: true.
        /// If set, an additional chunk will be streamed before the data: [DONE] message. The usage field on this chunk
        /// shows the token usage statistics for the entire request, and the choices field will always be an empty
        /// array. All other chunks will also include a usage field, but with a null value.
        /// </summary>
        public object? StreamOptions { get; set; } = null;

        /// <summary>
        /// What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more
        /// random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend
        /// altering this or top_p but not both.
        /// </summary>
        [JsonPropertyName("temperature")]
        public float Temperature { get; set; } = (float)0.01;

        /// <summary>
        /// An alternative to sampling with temperature, called nucleus sampling, where the model considers the results
        /// of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability
        /// mass are considered.
        /// </summary>
        [JsonPropertyName("top_p")]
        public float? TopP { get; set; } = (float?)0.5;

        /// <summary>
        /// A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a
        /// list of functions the model may generate JSON inputs for. A max of 128 functions are supported.
        /// </summary>
        [JsonPropertyName("tools")]
        public object? Tools { get; set; } = null;

        /// <summary>
        /// Controls which (if any) tool is called by the model. none means the model will not call any tool and instead
        ///  generates a message. auto means the model can pick between generating a message or calling one or more
        /// tools. required means the model must call one or more tools. Specifying a particular tool via
        /// {"type": "function", "function": {"name": "my_function"}} forces the model to call that tool.
        /// none is the default when no tools are present. auto is the default if tools are present.
        /// </summary>
        [JsonPropertyName("tool_choice")]
        public object? ToolChoice { get; set; }

        /// <summary>
        /// Whether to enable parallel function calling during tool use.
        /// </summary>
        [JsonPropertyName("parallel_tool_calls")]
        public bool? ParallelToolCalls { get; set; } = true;

        /// <summary>
        /// A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse.
        /// </summary>
        [JsonPropertyName("user")]
        public string? User { get; set; }

        /// <summary>
        /// can be "en", "zh"
        /// </summary>
        [JsonPropertyName("language")]
        public string Language { get; set; } = "auto";
    }
}

